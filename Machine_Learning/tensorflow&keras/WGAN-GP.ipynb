{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN-GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sqlite3\n",
    "\n",
    "dataset = \n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X =\n",
    "\n",
    "print(train_X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GPU Diet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from tensorflow.config.experimental import list_physical_devices, set_memory_growth\n",
    "gpus = list_physical_devices('GPU')\n",
    "display(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "    \n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#critic\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LeakyReLU, Flatten, Dropout, Lambda, BatchNormalization, Input, MaxPooling1D, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "clear_session()\n",
    "\n",
    "# YOUR PARAMTERS\n",
    "#######################################################################################\n",
    "\n",
    "np.random.seed(1500)\n",
    "tf.random.set_seed(1500)\n",
    "\n",
    "kernel_size = 7\n",
    "pool_size = train_X.shape[1] - kernel_size + 1\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "def convolution_block(x, filters, size, strides):\n",
    "    \n",
    "    bn = BatchNormalization()(x)\n",
    "    l_relu = LeakyReLU()(bn)\n",
    "    conv1d = Conv1D(filters=filters, kernel_size=size, strides=strides, kernel_regularizer=regularizers.l1(1e-5),padding='same')(l_relu)\n",
    "    \n",
    "    return conv1d\n",
    "    \n",
    "def create_critic(input_dim):\n",
    "    \n",
    "    # MODEL\n",
    "    #######################################################################################\n",
    "    \n",
    "    inp = Input(shape=input_dim)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=7, strides=1, kernel_regularizer=regularizers.l1(1e-5),padding='same')(inp)\n",
    "    convb1 = convolution_block(conv1, filters=64, size=7, strides=1)\n",
    "    convb2 = convolution_block(convb1, filters=64, size=7, strides=1)\n",
    "    convb2 = convolution_block(convb2, filters=32, size=7, strides=1)\n",
    "    add1 = Add()([conv1, convb2])\n",
    "    mp1 = MaxPooling1D(pool_size=input_dim[0])(add1)\n",
    "    \n",
    "    flatten = Flatten()(mp1)\n",
    "    \n",
    "    fc1 = Dense(32, kernel_regularizer=regularizers.l1(1e-5))(flatten)\n",
    "    l_relu1 = LeakyReLU()(fc1)\n",
    "    dropout1 = Dropout(rate=0.25)(l_relu1)\n",
    "    outp = Dense(1, kernel_regularizer=regularizers.l1(1e-5))(dropout1)\n",
    "    \n",
    "    model = Model(inp, outp)\n",
    "\n",
    "    #######################################################################################\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "critic = create_critic(train_X.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator\n",
    "from tensorflow.keras.layers import Conv1DTranspose, Reshape, Activation, UpSampling1D, ZeroPadding1D, Input, BatchNormalization, ReLU\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "z_dim = 100\n",
    "\n",
    "# YOUR PARAMTERS\n",
    "#######################################################################################\n",
    "\n",
    "design_len = 30\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "def create_generator(z_dim):\n",
    "    model = Sequential(name=\"generator\")\n",
    "    # MODEL\n",
    "    #######################################################################################\n",
    "    model.add(Input(z_dim))\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l1(1e-5)))\n",
    "#     model.add(LeakyReLU())\n",
    "    model.add(Dense(32, kernel_regularizer=regularizers.l1(1e-5)))\n",
    "    model.add(Reshape((1, 32)))\n",
    "    model.add(Conv1DTranspose(filters=4, kernel_size=design_len, strides=1, kernel_regularizer=regularizers.l1(1e-5)))\n",
    "\n",
    "    model.add(Activation(\"softmax\"))\n",
    "#     model.add(Lambda(gumbel_softmax)\n",
    "    \n",
    "#     model.add(Lambda(lambda x: x-.25))\n",
    "#     model.add(ZeroPadding1D(padding=(9, 9)))\n",
    "#     model.add(Lambda(lambda x: x+.25))\n",
    "\n",
    "    #######################################################################################\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "generator = create_generator(z_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "critic_opt = Adam(learning_rate = 0.0001, beta_1=0, beta_2=0.9)\n",
    "generator_opt = Adam(learning_rate = 0.0001, beta_1=0, beta_2=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_critic(seq, sample_weights):\n",
    "    batch_size = len(seq)\n",
    "    z = np.random.normal(0, 1, (batch_size, 100))\n",
    "    gp_lambda = 10\n",
    "    \n",
    "    with tf.GradientTape() as critic_tape:\n",
    "        critic.training = True\n",
    "        gen_seq = generator(z)\n",
    "        output_real = critic(seq)\n",
    "        output_fake = critic(gen_seq)\n",
    "        \n",
    "        #wgan loss\n",
    "        loss = K.mean(output_fake) - K.mean(output_real)\n",
    "        \n",
    "        e = tf.random.normal([batch_size, 1, 1], 0.0, 1.0)\n",
    "        x_hat = e * seq - (1 - e) * gen_seq\n",
    "    \n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            t.watch(x_hat)\n",
    "            c_hat = critic(x_hat)\n",
    "            \n",
    "        gradients = t.gradient(c_hat, [x_hat])\n",
    "        l2_norm = K.sqrt(K.sum(K.square(gradients), axis=[2,3]))\n",
    "        l2_norm = K.squeeze(l2_norm, axis=0)\n",
    "        gradient_penalty = K.sum(K.square((l2_norm-1.)), axis=[0])\n",
    "        \n",
    "        loss += gp_lambda*gradient_penalty\n",
    "        \n",
    "    g_critic = critic_tape.gradient(loss, critic.trainable_variables)\n",
    "    critic_opt.apply_gradients(zip(g_critic, critic.trainable_variables))\n",
    "    \n",
    "    return K.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(batch_size):\n",
    "    z = np.random.normal(0, 1, (batch_size, 100))\n",
    "    \n",
    "    with tf.GradientTape() as generator_tape:\n",
    "        generator.training = True\n",
    "        gen_seq = generator(z)\n",
    "        output_fake = critic(gen_seq)\n",
    "        \n",
    "        loss = - K.mean(output_fake)\n",
    "        \n",
    "    g_generator = generator_tape.gradient(loss, generator.trainable_variables)\n",
    "    generator_opt.apply_gradients(zip(g_generator, generator.trainable_variables))\n",
    "    \n",
    "    return K.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def train(dataset, importance, epoch):\n",
    "    for epoch in range(epoch):\n",
    "        g_loss = []\n",
    "        c_loss = []\n",
    "        \n",
    "        shuf = np.array([i for i in range(train_X.shape[0])])\n",
    "        random.shuffle(shuf)\n",
    "\n",
    "        dataset = dataset[shuf]\n",
    "        \n",
    "        for batch in tqdm(range(dataset.shape[0] // batch_size)):\n",
    "            real_X = dataset[batch * batch_size:(batch+1)*batch_size]\n",
    "            \n",
    "            loss_c = 0\n",
    "            for i in range(5):\n",
    "                loss_c += train_critic(real_X, 1)\n",
    "            loss_g = train_generator(batch_size)\n",
    "        \n",
    "            g_loss.append(loss_g)\n",
    "            c_loss.append(loss_c / 5)\n",
    "        \n",
    "        # loss & plot\n",
    "        print ('epoch {}'.format(epoch + 1))\n",
    "        print (epoch+1, \":\", \"g_loss: {}\".format(sum(g_loss)/len(g_loss)))\n",
    "        print (epoch+1, \":\", \"c_loss: {}\".format(sum(c_loss)/len(c_loss)))\n",
    "         \n",
    "\n",
    "        show_plot()\n",
    "        write_gen_fasta(epoch)\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    retun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(train_X, importance, 100)\n",
    "# train(train_X, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
